{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoMLPipe-BC Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs all aspects of the autoMLPipe-BC. Pipeline run parameters are set throughout the notebook at the start of each section. Manditory run parameters that must set for the pipeline to run properly are identified below.\n",
    "\n",
    "This notebook is set up to run 'as-is' on a 'demo' dataset from the UCI repository (HCC dataset) using only three modeling algorithms (so that it runs in a matter of minutes). Users will need to update run parameters in each phase below to adapt pipeline to their own file paths and run parameter needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ExploratoryAnalysisMain\n",
    "import ExploratoryAnalysisJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Set Run Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mandatory Parameters to Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_run = False #Leave true to run the local demo dataset (without specifying any datapaths), make False to specify a different data folder path below\n",
    "\n",
    "#Target dataset folder path(must include one or more .txt or .csv datasets)\n",
    "data_path = \"/home/meyer/code-project/AutoML-Pipe/AutoML-Pipe/data_input\" \n",
    "\n",
    "#Output foder path: where to save pipeline outputs (must be updated for a given user)\n",
    "output_path = \"/home/meyer/code-project/AutoML-Pipe/AutoML-Pipe/results\"\n",
    "#output_path = 'C:/Users/ryanurb/Documents/Analysis/AutoMLPipe_Experiments'\n",
    "\n",
    "#Unique experiment name - folder created for this analysis within output folder path\n",
    "experiment_name = 'myoxia'\n",
    "\n",
    "# Data Labels\n",
    "class_label = 'conclusion' #i.e. class outcome column label\n",
    "instance_label = 'id' #If data includes instance labels, given respective column name here, otherwise put 'None'\n",
    "\n",
    "#Manually specify features to leave out of analysis, or which to treat as categorical (without using built in variable type detector)\n",
    "# ignore_features = [\"patient_id\",\"expert_id\",\"biopsie_id\",\"muscle_prelev\",\"date_envoie\",\"mutation\",\"pheno_terms\",\"comment\",\"BOQA_prediction\",\"BOQA_prediction_score\",\"datetime\"] #list of column names to exclude from the analysis (only insert column names if needed, otherwise leave empty)\n",
    "ignore_features = [\"patient_id\",\"expert_id\",\"biopsie_id\",\"age_biopsie\", \"gene_diag\",\"muscle_prelev\",\"date_envoie\",\"mutation\",\"pheno_terms\",\"comment\",\"BOQA_prediction\",\"BOQA_prediction_score\",\"datetime\"]\n",
    "categorical_feature_headers = [] # empty list for 'auto-detect' otherwise list feature names to be treated as categorical. Only impacts algorithms that can take variable type into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters to Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_partitions = 10\n",
    "partition_method = 'S' # S or R or M for stratified, random, or matched, respectively\n",
    "match_label = 'None' # only applies when M selected for partition-method; indicates column label with matched instance ids' \n",
    "\n",
    "categorical_cutoff = 6 # number of unique values after which a variable is considered to be quantitative vs categorical\n",
    "sig_cutoff = 0.05 #significance cutoff used throughout pipeline\n",
    "export_exploratory_analysis = 'True' # run and export basic exploratory analysis files, i.e. unique value counts, missingness counts, class balance barplot\n",
    "export_feature_correlations = 'True' # run and export feature correlation analysis (yields correlation heatmap)\n",
    "export_univariate_plots = 'True' # export univariate analysis plots (note: univariate analysis still output by default)\n",
    "topFeatures = 10 #Number of top features to report in notebook for univariate analysis\n",
    "random_state = 42 # sets a specific random seed for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set and Report Data Folder Path as Specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_run:\n",
    "    wd_path = os.getcwd() #Working directory path automatically detected\n",
    "    wd_path = wd_path.replace('\\\\','/')\n",
    "    data_path = wd_path+'/DemoData'\n",
    "print(\"Data Folder Path: \"+data_path)\n",
    "jupyterRun = 'True' #Leave True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Conduct Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExploratoryAnalysisMain.makeDirTree(data_path,output_path,experiment_name,jupyterRun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1.1: Clean our dataset and encode the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "raw_df = pd.read_csv(\"EHRoes/text_reports.csv\")\n",
    "\n",
    "# Modify Onto features values: on modifie les valeur: 0:-1, puis on modifie les N/A en 0\n",
    "# Get list of ontology columns\n",
    "onto_col = []\n",
    "for column in raw_df.columns.to_list():\n",
    "    if column[0:4]==\"MHO:\":\n",
    "        onto_col.append(column)\n",
    "\n",
    "#  Simplify quantity\n",
    "raw_df[onto_col] = raw_df[onto_col].replace({0:0, 0.25:1, 0.5:1, 0.75:1})\n",
    "\n",
    "# One-hot Encode Cat Variable\n",
    "# Needed if gene_diag included in analysis\n",
    "# raw_df = pd.get_dummies(raw_df, columns=[\"gene_diag\"])\n",
    "\n",
    "# Label Encode Conclusion\n",
    "raw_df[\"conclusion\"].replace({\"COM_CCD\":\"COM\", \"UNCLEAR\":np.NaN,\"NM_CAP\":\"NM\",\"CFTD\":np.NaN,\"COM_MMM\":\"COM\",\"NON_CM\":np.NaN, \"CM\":np.NaN}, inplace=True)\n",
    "le = preprocessing.LabelEncoder()\n",
    "fit_by = pd.Series([i for i in raw_df[\"conclusion\"].unique() if type(i) == str])\n",
    "le.fit(fit_by)\n",
    "raw_df[\"conclusion\"] = raw_df[\"conclusion\"].map(lambda x: le.transform([x])[0] if type(x)==str else x)\n",
    "print(\"Class Label: \")\n",
    "for index, value in enumerate(le.classes_):\n",
    "    print(\"Class Value: \", index, \" Class Name: \", value)\n",
    "\n",
    "# Delete columns with all missing values\n",
    "raw_df[onto_col].dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "raw_df[onto_col] = raw_df[onto_col].fillna(0)\n",
    "\n",
    "raw_df.to_csv(data_path+\"/input.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine file extension of datasets in target folder:\n",
    "file_count = 0\n",
    "unique_datanames = []\n",
    "for dataset_path in glob.glob(data_path+'/*'):\n",
    "    dataset_path = str(dataset_path).replace('\\\\','/')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print(dataset_path)\n",
    "    file_extension = dataset_path.split('/')[-1].split('.')[-1]\n",
    "    data_name = dataset_path.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n",
    "    if file_extension == 'txt' or file_extension == 'csv':\n",
    "        if data_name not in unique_datanames:\n",
    "            unique_datanames.append(data_name)\n",
    "            ExploratoryAnalysisJob.runExplore(dataset_path,output_path+'/'+experiment_name,cv_partitions,partition_method,categorical_cutoff,export_exploratory_analysis,export_feature_correlations,export_univariate_plots,class_label,instance_label,match_label,random_state,ignore_features,categorical_feature_headers,sig_cutoff,jupyterRun)\n",
    "            file_count += 1\n",
    "\n",
    "if file_count == 0: #Check that there was at least 1 dataset\n",
    "    raise Exception(\"There must be at least one .txt or .csv dataset in data_path directory\")\n",
    "    \n",
    "with open(output_path+'/'+experiment_name+'/'+'metadata.csv',mode='w', newline=\"\") as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow([\"DATA LABEL\", \"VALUE\"])\n",
    "    writer.writerow([\"class label\",class_label])\n",
    "    writer.writerow([\"instance label\", instance_label])\n",
    "    writer.writerow([\"match label\", match_label])\n",
    "    writer.writerow([\"random state\",random_state])\n",
    "    writer.writerow([\"categorical cutoff\",categorical_cutoff])\n",
    "    writer.writerow([\"statistical significance cutoff\",sig_cutoff])\n",
    "    writer.writerow([\"cv partitions\",cv_partitions])\n",
    "    writer.writerow([\"partition method\",partition_method])\n",
    "    writer.writerow([\"ignored features\",ignore_features])\n",
    "    writer.writerow([\"specified categorical variables\",categorical_feature_headers])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataPreprocessingJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = 'False' #perform data scaling?\n",
    "impute_data = 'True' # perform missing value data imputation? (required for most ML algorithms if missing data is present)\n",
    "overwrite_cv = 'True' # overwrites earlier cv datasets with new scaled/imputed ones\n",
    "multi_impute = 'True' # applies multivariate imputation to quantitative features, otherwise uses mean imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 Conduct Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n",
    "dataset_paths.remove('metadata.csv')\n",
    "for dataset_directory_path in dataset_paths:\n",
    "    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n",
    "    for cv_train_path in glob.glob(full_path+\"/CVDatasets/*Train.csv\"):\n",
    "        cv_train_path = str(cv_train_path).replace('\\\\','/')\n",
    "        cv_test_path = cv_train_path.replace(\"Train.csv\",\"Test.csv\")\n",
    "        DataPreprocessingJob.job(cv_train_path,cv_test_path,output_path+'/'+experiment_name,scale_data,impute_data,overwrite_cv,categorical_cutoff,class_label,instance_label,random_state,multi_impute)\n",
    "\n",
    "metadata = pd.read_csv(output_path + '/' + experiment_name + '/' + 'metadata.csv').values\n",
    "if metadata.shape[0] == 10: #Only update if metadata below hasn't been added before (i.e. in a previous phase 2 run)\n",
    "    with open(output_path + '/' + experiment_name + '/' + 'metadata.csv',mode='a', newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"data scaling\",scale_data])\n",
    "        writer.writerow([\"data imputation\",impute_data])\n",
    "        writer.writerow([\"multivariate imputation\",multi_impute])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 Feature Importance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeatureImportanceJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_mutual_info = 'True' #do mutual information analysis\n",
    "do_multisurf = 'True' #do multiSURF analysis\n",
    "use_TURF = 'False' # use TURF wrapper around MultiSURF\n",
    "TURF_pct = 0.5 # proportion of instances removed in an iteration (also dictates number of iterations)\n",
    "njobs = -1 #number of cores dedicated to running algorithm; setting to -1 will use all available cores\n",
    "instance_subset = 2000 #sample subset size to use with multiSURF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 Conduct Feature Importance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n",
    "dataset_paths.remove('metadata.csv')\n",
    "dataset_paths.remove('jobsCompleted')\n",
    "for dataset_directory_path in dataset_paths:\n",
    "    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n",
    "    experiment_path = output_path+'/'+experiment_name\n",
    "\n",
    "    if eval(do_mutual_info) or eval(do_multisurf):\n",
    "        if not os.path.exists(full_path+\"/feature_selection\"):\n",
    "            os.mkdir(full_path+\"/feature_selection\")\n",
    "            \n",
    "    if eval(do_mutual_info):\n",
    "        if not os.path.exists(full_path+\"/feature_selection/mutualinformation\"):\n",
    "            os.mkdir(full_path+\"/feature_selection/mutualinformation\")\n",
    "        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n",
    "            cv_train_path = str(cv_train_path).replace('\\\\','/')\n",
    "            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'mi',njobs,use_TURF,TURF_pct)\n",
    "\n",
    "    if eval(do_multisurf):\n",
    "        if not os.path.exists(full_path+\"/feature_selection/multisurf\"):\n",
    "            os.mkdir(full_path+\"/feature_selection/multisurf\")\n",
    "        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n",
    "            cv_train_path = str(cv_train_path).replace('\\\\','/')\n",
    "            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'ms',njobs,use_TURF,TURF_pct)\n",
    "\n",
    "metadata = pd.read_csv(output_path + '/' + experiment_name + '/' + 'metadata.csv').values        \n",
    "if metadata.shape[0] == 13: #Only update if metadata below hasn't been added before (i.e. in a previous phase 2 run)\n",
    "    with open(output_path + '/' + experiment_name + '/' + 'metadata.csv',mode='a', newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"mutual information\",do_mutual_info])\n",
    "        writer.writerow([\"MultiSURF\", do_multisurf])\n",
    "        writer.writerow([\"TURF\",use_TURF])\n",
    "        writer.writerow([\"TURF cutoff\", TURF_pct])\n",
    "        writer.writerow([\"MultiSURF instance subset\", instance_subset])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeatureSelectionJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_to_keep = 2000 # max features to keep. None if no max\n",
    "filter_poor_features = 'True' # filter out the worst performing features prior to modeling\n",
    "top_results = 10 #number of top features to illustrate in figures\n",
    "export_scores = 'True' #export figure summarizing average feature importance scores over cv partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 Conduct Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n",
    "dataset_paths.remove('metadata.csv')\n",
    "dataset_paths.remove('jobsCompleted')\n",
    "for dataset_directory_path in dataset_paths:\n",
    "    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n",
    "    FeatureSelectionJob.job(full_path,do_mutual_info,do_multisurf,max_features_to_keep,filter_poor_features,top_results,export_scores,class_label,instance_label,cv_partitions,overwrite_cv,jupyterRun)\n",
    "\n",
    "metadata = pd.read_csv(output_path + '/' + experiment_name + '/' + 'metadata.csv').values\n",
    "if metadata.shape[0] == 18: #Only update if metadata below hasn't been added before\n",
    "    with open(output_path + '/' + experiment_name + '/' + 'metadata.csv',mode='a', newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"max features to keep\",max_features_to_keep])\n",
    "        writer.writerow([\"filter poor features\", filter_poor_features])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 ML Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ModelJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Model Algorithm Options (individual hyperparameter options can be adjusted below)\n",
    "do_NB = 'True'       #run naive bayes modeling\n",
    "do_LR = 'True'       #run logistic regression modeling\n",
    "do_DT = 'True'     #run decision tree modeling\n",
    "do_RF = 'True'       #run random forest modeling\n",
    "do_GB = 'True'       #run k-neighbors classifier modeling\n",
    "do_XGB = 'True'      #run XGBoost modeling\n",
    "do_LGB = 'True'     #run LGBoost modeling\n",
    "do_SVM = 'True'     #run support vector machine modeling\n",
    "do_ANN = 'True'      #run artificial neural network modeling\n",
    "do_KN = 'True'       #run gradient boosting modeling\n",
    "do_eLCS = 'False'     #run eLCS modeling (a basic supervised-learning learning classifier system)\n",
    "do_XCS = 'False'      #run XCS modeling (a supervised-learning-only implementation of the best studied learning classifier system)\n",
    "do_ExSTraCS = 'True' #run ExSTraCS modeling (a learning classifier system designed for biomedical data mining)\n",
    "\n",
    "#Other Analysis Parameters\n",
    "training_subsample = 0  #for long running algos, option to subsample training set (0 for no subsample) Limit Sample Size Used to train algorithms that do not scale up well in large instance spaces (i.e. XGB,SVM,KN,ANN,and LR to a lesser degree) and depending on 'instances' settings, ExSTraCS, eLCS, and XCS)\n",
    "use_uniform_FI = 'True' #overides use of any available feature importances estimate methods from models, instead using permutation_importance uniformly\n",
    "primary_metric = 'balanced_accuracy'\n",
    "\n",
    "#XGBoost N Class\n",
    "classes = np.unique(raw_df[class_label].dropna())\n",
    "num_classes = len(classes)\n",
    "\n",
    "#Hyperparameter Sweep Options\n",
    "n_trials = 100   #number of bayesian hyperparameter optimization trials using optuna\n",
    "timeout = 300    #seconds until hyperparameter sweep stops running new trials (Note: it may run longer to finish last trial started)\n",
    "export_hyper_sweep_plots = 'False' #Export hyper parameter sweep plots from optuna\n",
    "\n",
    "#Learning classifier system specific options (ExSTraCS, eLCS, XCS)\n",
    "do_lcs_sweep = 'False' #do LCS hyperparam tuning or use below params\n",
    "nu = 1               #fixed LCS nu param\n",
    "iterations = 100000  #fixed LCS # learning iterations param\n",
    "N = 500             #fixed LCS rule population maximum size param\n",
    "lcs_timeout = 1200 #seconds until hyperparameter sweep stops for LCS algorithms (evolutionary algorithms often require more time for a single run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(random_state,do_lcs_sweep,nu,iterations,N):\n",
    "    param_grid = {}\n",
    "    #######EDITABLE CODE################################################################################################\n",
    "    # Naive Bayes - no hyperparameters\n",
    "    \n",
    "    # Logistic Regression - can take a longer while in larger instance spaces\n",
    "    param_grid_LR = {'penalty': ['l2', 'l1'],'C': [1e-5, 1e5],'dual': [True, False],\n",
    "                     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                     'class_weight': [None, 'balanced'],'max_iter': [10, 1000],\n",
    "                     'random_state':[random_state]}\n",
    "    # Decision Tree\n",
    "    param_grid_DT = {'criterion': ['gini', 'entropy'],'splitter': ['best', 'random'],'max_depth': [1, 30],\n",
    "                     'min_samples_split': [2, 50],'min_samples_leaf': [1, 50],'max_features': [None, 'auto', 'log2'],\n",
    "                     'class_weight': [None, 'balanced'],\n",
    "                     'random_state':[random_state]}\n",
    "    # Random Forest\n",
    "    param_grid_RF = {'n_estimators': [10, 1000],'criterion': ['gini', 'entropy'],'max_depth': [1, 30],\n",
    "                     'min_samples_split': [2, 50],'min_samples_leaf': [1, 50],'max_features': [None, 'auto', 'log2'],\n",
    "                     'bootstrap': [True],'oob_score': [False, True],'class_weight': [None, 'balanced'],\n",
    "                     'random_state':[random_state]}\n",
    "    # GB\n",
    "    param_grid_GB = {'n_estimators': [10, 1000],'loss': ['deviance', 'exponential'], 'learning_rate': [.0001, 0.3], \n",
    "                     'min_samples_leaf': [1, 50],'min_samples_split': [2, 50], 'max_depth': [1, 30],\n",
    "                     'random_state':[random_state]}\n",
    "    # XG Boost - not great for large instance spaces (limited completion). note: class weight balance is included as option internally\n",
    "    param_grid_XGB = {'booster': ['gbtree'],'objective': ['multi:softmax'],'verbosity': [0],'reg_lambda': [1e-8, 1.0],\n",
    "                      'alpha': [1e-8, 1.0],'eta': [1e-8, 1.0],'gamma': [1e-8, 1.0],'max_depth': [1, 30],\n",
    "                      'grow_policy': ['depthwise', 'lossguide'],'n_estimators': [10, 1000],'min_samples_split': [2, 50],\n",
    "                      'min_samples_leaf': [1, 50],'subsample': [0.5, 1.0],'min_child_weight': [0.1, 10],\n",
    "                      'colsample_bytree': [0.1, 1.0],'nthread':[1],'random_state':[random_state], 'num_class':[num_classes]}\n",
    "\n",
    "    # LG Boost - note: class weight balance is included as option internally (still takes a while on large instance spaces)\n",
    "    param_grid_LGB = {'objective': ['multiclass'],'metric': [''],'verbose': [-1],'boosting_type': ['gbdt'],\n",
    "                      'num_leaves': [2, 256],'max_depth': [1, 30],'reg_alpha': [1e-8, 10.0],'reg_lambda': [1e-8, 10.0],\n",
    "                      'colsample_bytree': [0.4, 1.0],'subsample': [0.4, 1.0],'subsample_freq': [1, 7],\n",
    "                      'min_child_samples': [5, 100],'n_estimators': [10, 1000],'n_jobs':[1],'random_state':[random_state]}\n",
    "    # SVM - not approppriate for large instance spaces\n",
    "    param_grid_SVM = {'kernel': ['linear', 'poly', 'rbf'],'C': [0.1, 1000],'gamma': ['scale'],'degree': [1, 6],\n",
    "                      'probability': [True],'class_weight': [None, 'balanced'],'random_state':[random_state]}\n",
    "    # ANN - bad for large instances spaces\n",
    "    param_grid_ANN = {'n_layers': [1, 3],'layer_size': [1, 100],'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                      'learning_rate': ['constant', 'invscaling', 'adaptive'],'momentum': [.1, .9],\n",
    "                      'solver': ['sgd', 'adam'],'batch_size': ['auto'],'alpha': [0.0001, 0.05],'max_iter': [200],\n",
    "                      'random_state':[random_state]}\n",
    "    # KN - not appropriate for large instance spaces\n",
    "    param_grid_KN = {'n_neighbors': [1, 100], 'weights': ['uniform', 'distance'], 'p': [1, 5],\n",
    "                     'metric': ['euclidean', 'minkowski']}\n",
    "    if eval(do_lcs_sweep):\n",
    "        # eLCS\n",
    "        param_grid_eLCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n",
    "                           'random_state':[random_state]}\n",
    "        # XCS\n",
    "        param_grid_XCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n",
    "                          'random_state':[random_state]}\n",
    "        # ExSTraCS\n",
    "        param_grid_ExSTraCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n",
    "                               'random_state':[random_state],'rule_compaction':[None]}\n",
    "    else:\n",
    "        # eLCS\n",
    "        param_grid_eLCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state]}\n",
    "        # XCS\n",
    "        param_grid_XCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state]}\n",
    "        # ExSTraCS\n",
    "        param_grid_ExSTraCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state], \n",
    "                               'rule_compaction': [None]}\n",
    "\n",
    "    ####################################################################################################################\n",
    "    param_grid['naive_bayes'] = {}\n",
    "    param_grid['logistic_regression'] = param_grid_LR\n",
    "    param_grid['decision_tree'] = param_grid_DT\n",
    "    param_grid['random_forest'] = param_grid_RF\n",
    "    param_grid['gradient_boosting'] = param_grid_GB\n",
    "    param_grid['XGB'] = param_grid_XGB\n",
    "    param_grid['LGB'] = param_grid_LGB\n",
    "    param_grid['SVM'] = param_grid_SVM\n",
    "    param_grid['ANN'] = param_grid_ANN\n",
    "    param_grid['k_neighbors'] = param_grid_KN\n",
    "    param_grid['eLCS'] = param_grid_eLCS\n",
    "    param_grid['XCS'] = param_grid_XCS\n",
    "    param_grid['ExSTraCS'] = param_grid_ExSTraCS\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructs the list of algorithms that will be run\n",
    "algorithms = []\n",
    "if eval(do_NB):\n",
    "    algorithms.append('naive_bayes')\n",
    "if eval(do_LR):\n",
    "    algorithms.append(\"logistic_regression\")\n",
    "if eval(do_DT):\n",
    "    algorithms.append(\"decision_tree\")\n",
    "if eval(do_RF):\n",
    "    algorithms.append('random_forest')\n",
    "if eval(do_GB):\n",
    "    algorithms.append('gradient_boosting')\n",
    "if eval(do_XGB):\n",
    "    algorithms.append('XGB')\n",
    "if eval(do_LGB):\n",
    "    algorithms.append('LGB')\n",
    "if eval(do_SVM):\n",
    "    algorithms.append('SVM')\n",
    "if eval(do_ANN):\n",
    "    algorithms.append('ANN')\n",
    "if eval(do_KN):\n",
    "    algorithms.append('k_neighbors')\n",
    "if eval(do_eLCS):\n",
    "    algorithms.append('eLCS')\n",
    "if eval(do_XCS):\n",
    "    algorithms.append('XCS')\n",
    "if eval(do_ExSTraCS):\n",
    "    algorithms.append('ExSTraCS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 Conduct ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ModelJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n",
    "dataset_paths.remove('metadata.csv')\n",
    "dataset_paths.remove('jobsCompleted')\n",
    "for dataset_directory_path in dataset_paths:\n",
    "    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n",
    "    if not os.path.exists(full_path+'/models'):\n",
    "        os.mkdir(full_path+'/models')\n",
    "    if not os.path.exists(full_path+'/model_evaluation'):\n",
    "        os.mkdir(full_path+'/model_evaluation')\n",
    "    if not os.path.exists(full_path+'/models/pickledModels'):\n",
    "        os.mkdir(full_path+'/models/pickledModels')\n",
    "\n",
    "    for cvCount in range(cv_partitions):\n",
    "        train_file_path = full_path+'/CVDatasets/'+dataset_directory_path+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n",
    "        test_file_path = full_path + '/CVDatasets/' + dataset_directory_path + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n",
    "        for algorithm in algorithms:\n",
    "            #Get hyperparameter grid\n",
    "            param_grid = hyperparameters(random_state,do_lcs_sweep,nu,iterations,N)[algorithm]\n",
    "            ModelJob.runModel(algorithm,train_file_path,test_file_path,full_path,n_trials,timeout,lcs_timeout,export_hyper_sweep_plots,instance_label,class_label,random_state,cvCount,filter_poor_features,do_lcs_sweep,nu,iterations,N,training_subsample,use_uniform_FI,primary_metric,param_grid)\n",
    "\n",
    "metadata = pd.read_csv(output_path + '/' + experiment_name + '/' + 'metadata.csv').values\n",
    "if metadata.shape[0] == 20: #Only update if metadata below hasn't been added before\n",
    "    with open(output_path + '/' + experiment_name + '/' + 'metadata.csv', mode='a', newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"NB\", str(do_NB)])\n",
    "        writer.writerow([\"LR\", str(do_LR)])\n",
    "        writer.writerow([\"DT\", str(do_DT)])\n",
    "        writer.writerow([\"RF\", str(do_RF)])\n",
    "        writer.writerow([\"GB\",str(do_GB)])\n",
    "        writer.writerow([\"XGB\", str(do_XGB)])\n",
    "        writer.writerow([\"LGB\", str(do_LGB)])\n",
    "        writer.writerow([\"SVM\", str(do_SVM)])\n",
    "        writer.writerow([\"ANN\", str(do_ANN)])\n",
    "        writer.writerow([\"KN\", str(do_KN)])\n",
    "        writer.writerow([\"eLCS\", str(do_eLCS)])\n",
    "        writer.writerow([\"XCS\",str(do_XCS)])\n",
    "        writer.writerow([\"ExSTraCS\",str(do_ExSTraCS)])\n",
    "        writer.writerow([\"primary metric\",primary_metric])\n",
    "        writer.writerow([\"training subsample for KN,ANN,SVM,and XGB\",training_subsample])\n",
    "        writer.writerow([\"uniform feature importance estimation (models)\",use_uniform_FI])\n",
    "        writer.writerow([\"hypersweep number of trials\",n_trials])\n",
    "        writer.writerow([\"hypersweep timeout\",timeout])\n",
    "        writer.writerow(['do LCS sweep',do_lcs_sweep])\n",
    "        writer.writerow(['nu', nu])\n",
    "        writer.writerow(['training iterations', iterations])\n",
    "        writer.writerow(['N (rule population size)', N])\n",
    "        writer.writerow([\"LCS hypersweep timeout\",lcs_timeout])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6 Statistics (Stats Summaries, Figures, Statistical Comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 6 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatsJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 6 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC = 'True' #Plot ROC curves individually for each algorithm including all CV results and averages\n",
    "plot_PRC = 'True' #Plot PRC curves individually for each algorithm including all CV results and averages\n",
    "plot_FI_box = 'True' #Plot box plot summaries comparing algorithms for each metric\n",
    "plot_metric_boxplots = 'True' #Plot feature importance boxplots for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 6 Conduct Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(do_algo,encodedAlgos):\n",
    "    if eval(do_algo):\n",
    "        encodedAlgos += '1'\n",
    "    else:\n",
    "        encodedAlgos += '0'\n",
    "    return encodedAlgos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encodedAlgos = ''\n",
    "encodedAlgos = encode(do_NB, encodedAlgos)\n",
    "encodedAlgos = encode(do_LR,encodedAlgos)\n",
    "encodedAlgos = encode(do_DT, encodedAlgos)\n",
    "encodedAlgos = encode(do_RF, encodedAlgos)\n",
    "encodedAlgos = encode(do_GB, encodedAlgos)\n",
    "encodedAlgos = encode(do_XGB, encodedAlgos)\n",
    "encodedAlgos = encode(do_LGB, encodedAlgos)\n",
    "encodedAlgos = encode(do_SVM, encodedAlgos)\n",
    "encodedAlgos = encode(do_ANN, encodedAlgos)\n",
    "encodedAlgos = encode(do_KN, encodedAlgos)\n",
    "encodedAlgos = encode(do_eLCS, encodedAlgos)\n",
    "encodedAlgos = encode(do_XCS, encodedAlgos)\n",
    "encodedAlgos = encode(do_ExSTraCS, encodedAlgos)\n",
    "\n",
    "# Iterate through datasets\n",
    "dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n",
    "dataset_paths.remove('metadata.csv')\n",
    "dataset_paths.remove('jobsCompleted')\n",
    "for dataset_directory_path in dataset_paths:\n",
    "    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n",
    "    StatsJob.job(full_path,encodedAlgos,plot_ROC,plot_PRC,plot_FI_box,class_label,instance_label,cv_partitions,plot_metric_boxplots,primary_metric,top_results,sig_cutoff,jupyterRun)\n",
    "    #Stats(full_path,encodedAlgos,plot_ROC,plot_PRC,plot_FI_box,class_label,instance_label,cv_partitions,primary_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7 Dataset Comparison (only if > 1 dataset was analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 7 Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataCompareJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 7 Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 7 Conduct Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset_paths) > 1:\n",
    "    DataCompareJob.job(output_path+'/'+experiment_name,sig_cutoff,jupyterRun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Training Report Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PDF_ReportTrainJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = output_path+'/'+experiment_name\n",
    "PDF_ReportTrainJob.job(experiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Models to Replication Data\n",
    "This section will fail to run if a replication dataset is not available or left unspecified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Additional Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ApplyModelJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Run Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mandatory Parameters to Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_data_path = \"C:/Users/ryanu/OneDrive/Documents/GitHub/AutoMLPipe-BC/DemoRepData\" #as a test we use same copied dataset (no true replication data available)\n",
    "# data_path = \"C:/Users/ryanu/OneDrive/Documents/GitHub/AutoMLPipe-BC/DemoData/hcc-data_example.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if demo_run:\n",
    "#     wd_path = os.getcwd() #Working directory path automatically detected\n",
    "#     wd_path = wd_path.replace('\\\\','/')\n",
    "#     rep_data_path = wd_path+'/DemoRepData'\n",
    "#     data_path = wd_path+'/DemoData/hcc-data_example.csv'\n",
    "# print(\"Replication Data Folder Path: \"+rep_data_path)\n",
    "# print(\"Dataset Path: \"+data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct Application of Models to Replication Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = data_path.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n",
    "# full_path = output_path + \"/\" + experiment_name + \"/\" + data_name #location of folder containing models respective training dataset\n",
    "\n",
    "# if not os.path.exists(full_path+\"/applymodel\"):\n",
    "#     os.mkdir(full_path+\"/applymodel\")\n",
    "\n",
    "# #Determine file extension of datasets in target folder:\n",
    "# file_count = 0\n",
    "# unique_datanames = []\n",
    "# for datasetFilename in glob.glob(rep_data_path+'/*'):\n",
    "#     datasetFilename = str(datasetFilename).replace('\\\\','/')\n",
    "        \n",
    "#     file_extension = datasetFilename.split('/')[-1].split('.')[-1]\n",
    "#     apply_name = datasetFilename.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n",
    "#     if not os.path.exists(full_path+\"/applymodel/\"+apply_name):\n",
    "#         os.mkdir(full_path+\"/applymodel/\"+apply_name)\n",
    "\n",
    "#     if file_extension == 'txt' or file_extension == 'csv':\n",
    "#         if apply_name not in unique_datanames:\n",
    "#             unique_datanames.append(apply_name)\n",
    "#             ApplyModelJob.job(datasetFilename,full_path,class_label,instance_label,categorical_cutoff,sig_cutoff,cv_partitions,scale_data,impute_data,do_LR,do_DT,do_RF,do_NB,do_XGB,do_LGB,do_SVM,do_ANN,do_ExSTraCS,do_eLCS,do_XCS,do_GB,do_KN,primary_metric,data_path,match_label,plot_ROC,plot_PRC,plot_metric_boxplots,export_feature_correlations,jupyterRun,multi_impute)\n",
    "#             file_count += 1\n",
    "\n",
    "# if file_count == 0: #Check that there was at least 1 dataset\n",
    "#     raise Exception(\"There must be at least one .txt or .csv dataset in rep_data_path directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Apply Report Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PDF_ReportApplyJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_path = output_path+'/'+experiment_name\n",
    "# PDF_ReportApplyJob.job(experiment_path,rep_data_path,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
